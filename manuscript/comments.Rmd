# Elaborating on gene- and population-specific batch effects

Obviously, you do get stochastic population-wide differences, e.g., due to culture conditions affecting all cells of a population in a gene-specific manner.
Otherwise, if you didn't, it would all cancel out for millions of cells and bulk RNA-seq would have Poisson-level variability.

Note that there's no requirement for the batch effect variable to be identically distributed across genes.
It's just simulated that way for simplicity; I've actually adapted some of the simulations to have variable batch effects, and it works fine.

# Why not Bayesian methods?

BASiCS requires spike-ins, that the other methods don't use, so it's not entirely comparable.
Kharchenko's method generates z-scores and MLEs, but no p-values.

# Smacking down random effect models

See `failsim/extra` for improvement of the consensus estimate with an increasing number of plates, and see Figure S2 for an improvement in GLMM performance with more plates.
However, voom doesn't improve with more plates in Figure S2.
I suspect it's a closely related reason; that voom needs to estimate a consensus correlation, and that's not appropriate in these simulations.
In particular, the plate effect variance is constant for each gene, but its value as a proportion of the total variance is not - see failsim/extra for an example.
Note that discreteness is secondary, as voom itself does fine without a plate effect.

It's also worth noting that GLMMs and duplicateCorrelation assume a homogeneous (i.i.d.) effect for each block/plate.
This is the case for our framework, but it'll fall apart if you have systematic differences between plates that aren't modelled.
Summing doesn't really help either, as there's an equivalent assumption that summed observations are homogeneous.
You would need to include additional factors to block on any systematic effects that might be present.

# Independence of summed counts 

## Justification

You can model the count sum for each plate as being independently sampled from some NB-like distribution, conditional on the plate effect.
No dependencies should be observed in sampling between plates as they are separate experimental units (i.e., no need to consider joint sampling from a multivariate distribution).
The plate effect itself is also independent across plates - thus, the full count sum should also be independent between plates.

The only possible dependencies in the count sums should be caused by unmodelled differences in the means between plates.
Any differences in means due to library size/cell numbers can be normalized out as they apply for all genes.
However, this does require that the population structure is the same across replicate plates.

## Effect of population structure

If the population structure is the same between plates, the expectation of the count sum will be equal to the true sum of expression across subpopulations for each plate.
Then, the count sum for each plate will be (modelled as being) independently sampled from a distribution with an independent batch effect per plate.
If the population structure changes between plates, then one could model it as part of the batch effect (if variation in composition should be random across plates).
However, because the structure affects all genes, you'd get correlations between genes, which might lead to overestimation of the prior d.f. in edgeR.
I guess this is somewhat pathological as your "replicate plates" should have fairly similar subpopulations.

# Normalization on the summed counts

Consider a situation where you have cell types that differ quite dramatically in RNA content, and no library quantification is performed during scRNA-seq.
Summing the libraries together will preserve these differences, much like in bulk RNA sequencing.
On the other hand, normalizing on the single cells will usually remove differences in content.
This can result in a change in the log-fold change between conditions when comparing DE analyses with and without summation.
Unfortunately, there is no simple solution to this discrepancy that preserves the count-based nature of the data after summation.
In fact, it's not clear which method produces the correct log-fold change in the cases where they disagree.
(This should only occur when subsets of cells change in total RNA content, and only for genes that are already DE in those cells compared to others.)

# Interpreting DE genes from single-cell data

## Handling population structure

You could directly try to characterise the subpopulations with clustering on the DE genes.
The benefit is that with a formal DE analysis, you get proper type I error rate control.
This provides statistical rigour that complements the interpretability of the clusters.
One approach would be to identify DE genes and cluster on them to determine what substructure is driving them. 

Of course, one could argue that it would be better to split the subpopulations before the DE analysis.
However, this depends on being able to correctly identify empirical subpopulations.
This is not a trivial task (or, perhaps, even possible in any statistically rigorous manner).

## Differences between DE and marker genes

DE genes between subpopulations are not necessarily marker genes.
The latter requires that the gene be consistently expressed (or not) in all cells of one subpopulation compared to the other.
This means that the variance needs to be modelled across cells.
For DE genes, any change will do, even if it only appears in a small percentage of cells of the subpopulation.
This requires modelling of the variance across replicate instances of the entire subpopulation.

That being said, testing for differential expression is not the best approach to identify marker genes.
If the difference is strong enough, the null hypothesis will be rejected, regardless of how variable the expression values might be.
One might expect that DE analyses would at least rank good markers more highly if they have, e.g., strong log-fold changes and low variability.
This is true to some extent - however, the implicit balance between variability and log-fold change is not well defined.
A DE analysis will happily favour low-variability genes with weak DE - usually high-abundance genes expressed in both groups - that wouldn't be good markers.
Conversely, good candidates that are heavily affected by technical noise may be triaged out.

```{r}
set.seed(100)
library(statmod)
ngroup <- 50
design <- model.matrix(~rep(LETTERS[1:2], each=ngroup))
design0 <- cbind(rep(1, nrow(design)))

# Scenario 1: strong difference, highly variable.
# This is arguably a strong candidate for a marker gene.
y1 <- c(integer(ngroup), rnbinom(ngroup, mu=100, size=1)) 
sum(y1==0) - ngroup # only one cell in the second group is zero.
LR1 <- glmnb.fit(design0, y1, dispersion=1, offset=0)$deviance - 
       glmnb.fit(design, y1, dispersion=1, offset=0)$deviance

# Scenario 2: weaker difference, lowly variable.
# This is not a particularly good marker gene.
y2 <- c(rnbinom(ngroup, mu=50, size=100), 
        rnbinom(ngroup, mu=100, size=100)) 
LR2 <- glmnb.fit(design0, y2, dispersion=0.01, offset=0)$deviance - 
       glmnb.fit(design, y2, dispersion=0.01, offset=0)$deviance
 
# Despite that, the second gene has a greater LR than the first.
LR1
LR2
```

A rigorous test for markers would need to consider the variability explicitly, e.g., set a maximum overlap between the group-specific expression distributions.
This might be too conservative, though, due to high levels of technical noise.
If one perseveres with DE analyses, the only way to prune out bad markers is to inspect the distributions.
However, strong markers with large log-fold changes should appear at the top of the list, regardless of whether you do summation or model cell-to-cell variability.

# What did Kim do with ComBat?

Note that Kim used ComBat to remove batch effects, not plate effects.
Trying to do so won't work; ComBat will fail because it recognises (rightfully) that the plate effect is confounding with the groups in the design matrix.
In any case, ComBat seems to just regress out the plate effect, so even if you gave an all-intercept matrix to get it to run, you'd lose all DE.

# Summation in other contexts

## Without a grouping factor to sum over

Summation wouldn't work if you tried to use more gradated cluster identities (e.g., fractional membership or trajectories).
This is true but you'll be double-dipping to estimate the identities in the first place, so your statistics are probably broken to start with.
Of course, you could get independent covariates for each cell through indexed FACS.
However, this only seems to be relevant to experimental designs where you want to associate expression to some (protein) marker of interest.

## Using empirically identified subpopulations

There are several relevant points here for rigorous identification of DE genes after clutering into empirical subpopulations:

1. The empirical subpopulations must be identified in a manner that is blind to the sample labels.
This avoids systematic differences in clustering, caused by the differential presence of other subpopulations between conditions.
One would do this anyway, as it's too hard to match up clusters afterwards.
However, this may require some careful gene selection to avoid obtaining useless clusters that are condition-specific (e.g., due to strong DE in a few genes).
For example, you might cluster on pre-defined cell type markers to break up a heterogeneous population.
2. For contrasts _between_ subpopulations (e.g., blocking on replicate sample), it is safest to avoid testing the genes involved in clustering.
This avoids problems with double-dipping into the data, given that cells in different clusters will, by definition, differ in the genes used to cluster them.
If clustering is not precise, you may still get double-dipping when testing uninvolved genes, due to correlations in the errors with involved counterparts.
For example, you can get skewed axes from PCA that explain much more technical noise than expected by chance.
Testing for DE against these PCs would probably result in rejections purely due to correlation of errors.
3. For contrasts _between_ conditions and _blocking_ on the cluster identities, you will be overly conservative if you test the genes involved in clustering.
This is because, by definition, there will not be much difference in expression if the cells were put in the same cluster.
It's less clear whether there is a problem for uninvolved genes -- probably not, as blind clustering provides no information about the conditions.
The risk is that there is an artificial deflation of the variance, but this is should affect all conditions evenly (assuming similar numbers of cells).

The subsequent within-sample clusters can be summed together to avoid within-sample correlations.
This mitigates problems with Simpson's paradox while still handling the batch effect in a rigorous way.
Of course, it is not perfect, as you may still have problems with substructure within each cluster; but that's life.

Loss of control due to double-dipping may not seem like a problem for the top set of DE genes with very low p-values.
However, if the correlations are strong and the number of cells is high, there may be sufficient power to get very low p-values for true nulls.
I suggest that p-values for involved genes in comparisons between subpopulations should be treated as advisory - moreso than usual.

# Effect of summation on the variance

## Simplifying model assumptions

After summation, there's no need to worry about the nature of cell-to-cell variance across the population.
This avoids the need for complicated models involving zero inflation or multiple mixture components.
It's arguable that cells shouldn't be treated as replicates anyway in situations where there is strong substructure 
(assuming such structure induces correlations and reduces the effective residual degrees of freedom).

## Outlier protection after summation

Summation still protects against outliers as the variance of count sums driven from a single outlier observation is quite large.
However, the number of outlier cells to which this applies depends on the plate effect.
If the plate effect is large, then the impact of the conditional variance on the mean-var relationship is small.
This reduces the protection against outliers and improves the ranking of outlier-driven DE genes.
That being said, all DE will be less significant, so perhaps this is not a major concern.

## Reducing the variance with summation

Adding more i.i.d. NB variates together will generally lead to a lower dispersion.
The simulation below just checks that the equations in the Supplmentary Section 3 are correct:

```{r}
# To check that it's being done right:
n <- c(10, 20, 5)
b <- c(0.5, 1, 1.5)
all.vars <- all.means <- list()
counter <- 1L
disp <- 0.1
for (mu in c(20, 40, 60, 80, 100)) {
    collected <- list()
    for (plate in 1:10000) { 
        plate.effect <- exp(rnorm(1, -0.25, sd=sqrt(0.5)))
        counts <- rnbinom(sum(n), mu=plate.effect * rep(b, n) * mu, size=1/disp)
        collected[[plate]] <- sum(counts)
    }
    all.vars[[counter]] <- var(unlist(collected))
    all.means[[counter]] <- mean(unlist(collected))
    counter <- counter + 1L
}
plot(unlist(all.means), unlist(all.vars))
effect.dist <- exp(rnorm(1e6, -0.25, sd=sqrt(0.5)))
curve(col="red", add=TRUE,
x^2*var(effect.dist) + x + x^2*mean(effect.dist^2)*(disp*sum(n*b^2)/sum(n*b)^2))
```

However, note that just adding more cells doesn't guarantee that the dispersion will drop to zero.
This is because you could add a huge cell and the relationship would then be dominated by that cell in a plate-specific manner. 
This is a bit pathological, true, but nonetheless, it's possible.


